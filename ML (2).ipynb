{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vm9e1lzLDDP"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjVvtXF5JdjG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import re\n",
        "from src.utils import download_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COivZhtSJgAA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/train_subset1.csv')\n",
        "test_df = pd.read_csv('/content/test_subset1.csv')\n",
        "\n",
        "from src.utils import download_images\n",
        "import src.constants\n",
        "import pandas as pd\n",
        "\n",
        "# Load the train dataset\n",
        "train_df = pd.read_csv('/content/train_subset1.csv')\n",
        "\n",
        "# Download images\n",
        "download_images(train_df['image_link'].tolist(), 'images/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wAEQx-IJiTj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Image preprocessing function\n",
        "def load_and_preprocess_image(img_name, img_dir):\n",
        "    img_path = f\"{img_dir}/{img_name}\"\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img / 255.0  # Normalize the image\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "745Oq0GRJliX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load pre-trained EfficientNet model (without top layers)\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB8pMRf_Jn6s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to extract numeric value and unit from entity_value\n",
        "def extract_numeric_and_unit(value):\n",
        "    match = re.match(r\"([\\d\\.]+)\\s*(\\w+)\", value)\n",
        "    if match:\n",
        "        numeric_part = float(match.group(1))\n",
        "        unit_part = match.group(2).lower()\n",
        "        return numeric_part, unit_part\n",
        "    return None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "zKMq_PT8JtXo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess and extract features for train data\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True # Add this line to handle truncated images\n",
        "\n",
        "train_df['image_name'] = train_df['image_link'].apply(lambda url: url.split('/')[-1])\n",
        "train_image_features = np.array([base_model.predict(load_and_preprocess_image(img_name, '/content/images'))\n",
        "                                 for img_name in train_df['image_name']])\n",
        "train_image_features = train_image_features.reshape(len(train_df), -1)\n",
        "\n",
        "# Apply numeric value and unit extraction to train dataset\n",
        "train_df['numeric_value'], train_df['unit'] = zip(*train_df['entity_value'].map(extract_numeric_and_unit))\n",
        "train_df['combined_text'] = train_df['entity_name'] + ' ' + train_df['group_id'].astype(str) + ' ' + train_df['unit']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isna().sum()"
      ],
      "metadata": {
        "id": "ZcAwOIWNPtkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.fillna({'numeric_value': 0, 'unit': 'unknown','combined_text': 'unknown'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "k726aqo1QZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BERS7eK0JwoR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to encode text using BERT\n",
        "def encode_text(texts):\n",
        "    encoded_inputs = tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors='tf')\n",
        "    outputs = bert_model(encoded_inputs).last_hidden_state[:, 0, :]\n",
        "    return outputs\n",
        "\n",
        "# Encode the combined text using BERT for train data\n",
        "train_text_embeddings = encode_text(train_df['combined_text'])\n",
        "combined_train_features = np.concatenate([train_image_features, train_text_embeddings.numpy()], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_train_features"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tRcAEtOXQ7P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(combined_train_features, train_df['numeric_value'], test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "E7pCW8P7RJ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_val = y_val.values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "C84w7OHSRYv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08DK65HDJ1kS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize and train TabNet model\n",
        "tabnet_model = TabNetRegressor()\n",
        "tabnet_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    eval_name=['val'],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=50,\n",
        "    patience=5,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=16\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = tabnet_model.predict(X_val)"
      ],
      "metadata": {
        "id": "kPzntoLCXVXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define a threshold to classify predictions (you can choose a different method)\n",
        "threshold = train_df['numeric_value'].median()\n",
        "\n",
        "# Convert actual values to binary classes based on the threshold\n",
        "y_true = (y_val >= threshold).astype(int)\n",
        "\n",
        "# Convert predictions to binary classes based on the same threshold\n",
        "y_pred = (y_test_pred.flatten() >= threshold).astype(int)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "tSFGT3ycWPNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils import download_images\n",
        "import src.constants\n",
        "import pandas as pd\n",
        "\n",
        "# Load the train dataset\n",
        "\n",
        "\n",
        "# Download images\n",
        "download_images(test_df['image_link'].tolist(), 'images2/')"
      ],
      "metadata": {
        "id": "vQpJA9iLSjqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfLWlwQiJadL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess and extract features for test data\n",
        "test_df['image_name'] = test_df['image_link'].apply(lambda url: url.split('/')[-1])\n",
        "test_image_features = np.array([base_model.predict(load_and_preprocess_image(img_name, '/content/images2'))\n",
        "                                for img_name in test_df['image_name']])\n",
        "test_image_features = test_image_features.reshape(len(test_df), -1)\n",
        "\n",
        "# Combine text features for the test data\n",
        "test_df['combined_text'] = test_df['entity_name'] + ' ' + test_df['group_id'].astype(str)\n",
        "test_text_embeddings = encode_text(test_df['combined_text'])\n",
        "\n",
        "# Combine image and text features for the test data\n",
        "combined_test_features = np.concatenate([test_image_features, test_text_embeddings.numpy()], axis=1)\n",
        "# Predict the numeric values for the test data\n",
        "y_test_pred = tabnet_model.predict(combined_test_features)\n",
        "\n",
        "# Ensure y_test_pred is a 1D array\n",
        "y_test_pred_flat = y_test_pred.flatten()\n",
        "\n",
        "# Combine predictions with units for the final output\n",
        "test_df['predicted_entity_value'] = [f\"{pred:.2f}\" for pred, unit in zip(y_test_pred_flat, test_df['unit'])]\n",
        "\n",
        "\n",
        "# Prepare the final output in the required format\n",
        "test_output = test_df[['index', 'predicted_entity_value']]\n",
        "\n",
        "# Save the output to a CSV file\n",
        "test_output.to_csv('sample_test_out2.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1_7E3GMTllD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}